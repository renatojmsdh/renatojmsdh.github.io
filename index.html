<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5RCX2J3WYT"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5RCX2J3WYT');
</script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="Bluefish 2.2.7" >
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #982913;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    line-height: 140%
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="shortcut icon" href="https://www.u-bourgogne.fr/wp-content/uploads/2019/01/favicon.ico" type="image/x-icon" />
  <title>Renato Martins - Université de Bourgogne</title>  
  <meta http-equiv="Content-Type" content="text/html; charset=utf8">
  <link href="files/css.css" rel="stylesheet" type="text/css">
  </head> 
  <body>
<br/>
  <table border="0" cellpadding="0" cellspacing="0" align="center" width="1050">
    <tbody><tr>
    <td>
      <table border="0" cellpadding="20" cellspacing="0" align="center" width="100%">
      <tbody><tr>
            <td valign="middle" width="100%">
            <ul class="topmenu">
            <li class="topmenu"><a class="topmenu" href="index.html">Homepage</a></li>
            <li class="topmenu"><a class="topmenu" href="publications.html" title="Publications">Publications</a></li>
            <li class="topmenu"><a class="topmenu" href="colab.html" title="Collaborators">Collaborators</a></li>
            <li class="topmenu"><a class="topmenu" href="teaching.html" title="Teaching">Teaching</a></li>    
            <li class="topmenu"><a class="topmenu" href="https://github.com/renatojmsdh" target="_blank" title="Software">Software</a></li> 
            <li class="topmenu"><a class="topmenu" href="index.html" style="color:#982913;">.</a></li>

            </ul>
            </td>
        </tr>  
        <tbody><tr>    
        <td valign="middle" width="70%">
        <p align="left">
          <name>Renato Martins</name>
        </p> 
        
        <p>
        I am an Assistant Professor (Maître de Conférences) at <a href="https://www.u-bourgogne.fr" target="_blank">Université de Bourgogne</a>, 
        working in <a href="https://icb.u-bourgogne.fr/en/home-page/homepage/" target="_blank">CO2M team of the ICB</a>  CNRS Laboratory in Dijon. My teaching activies are in the <a href="https://esirem.u-bourgogne.fr/robotique-cobotique" target="_blank">Department 
          of Robotics</a> (Le Creusot) of the engineering school at the university (<a href="https://esirem.u-bourgogne.fr/" target="_blank">ESIREM</a>). I am also an Associate Member of the <a href="https://team.inria.fr/tangram/en/">TANGRAM team</a> at <a href="https://www.loria.fr/">INRIA Nancy/LORIA</a>.
       </p> 
        
        <p>Previously, I was a member of the <a href="https://imvia.u-bourgogne.fr/" target="_blank">Artifical Imagery and Vision" - ImViA</a>" laboratory until May 2023. I did my PhD at <a href="http://www.inria.fr" target="_blank">INRIA</a> Sophia Antipolis/France and at <a href="http://www.mines-paristech.fr" target="_blank">Ecole des Mines de Paris / Université Paris Sciences et Lettres</a>, where I was advised by <a href="https://team.inria.fr/acentauri/patrick-rives/" target="_blank">Dr. Patrick Rives</a>. Then, I was a post-doctoral researcher in the <a href="https://www.i3s.unice.fr/en" target="_blank">CNRS I3S</a> laboratory, in the <a href="https://team.inria.fr/acentauri/">ACENTAURI / CHORALE</a> group at INRIA and with the <a href="https://www.verlab.dcc.ufmg.br/" target="_blank">VeRLab</a> laboratory at <a href="https://www.ufmg.br" target="_blank">Universidade Federal de Minas Gerais</a>/Brazil, with whom I maintain tight research collaborations.</p> 
        <p>
            My research interests lie in Computer Vision, Machine Learning and Robot Vision, more specifically in the topics of 3D vision, geometric deep learning, human motion analysis, video prediction, and RGB-D image analysis and processing (perspective, omnidirectional).</p> 
        
        <p align="center">

            <a href="publications.html">Publications</a> &nbsp;/&nbsp;            
            <a href="colab.html">Collaborators</a> &nbsp;/&nbsp;
            <a href="https://github.com/renatojmsdh" target="_blank">Software</a> &nbsp;/&nbsp;
            <a href="https://scholar.google.com/citations?user=bs2yAMUAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp;/&nbsp; 
            <a href="mailto:renato .DOT. martins .AT. u-bourgogne .DOT. fr">Contact</a> 
        </p>
        </td>
        <td width="33%">       
        <img src="files/renato.martins.png" width="180">
        &nbsp; &nbsp; &nbsp;
        <a href="https://www.u-bourgogne.fr">
          <img style="margin-top:0.5cm;" src="files/ufbc.png" width="200">
        </a>        
        </td>
      </tr>
      </tbody></table>
      <table border="0" cellpadding="0" cellspacing="0" align="center" width="100%">
      <tbody><tr>
        <td valign="middle" width="100%">
          <heading style="margin-left:20">News</heading>
        </td>
      </tr>
       </tbody></table>
       <table border="0" cellpadding="0" cellspacing="0" align="center" width="100%">         
       <tbody>
        <ul class=space_list>
        <li> <font color='red'>[NEW]</font> 17.05.2023: <a href="https://cvpr2023.thecvf.com/Conferences/2023/OutstandingReviewers"> Outstanding Reviewer of CVPR 2023</a>  </li>
        <li> <font color='red'>[NEW]</font> 05.04.2023: <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a> paper accepted: "Deformation-Aware Local Features (DALF)" <a href="https://arxiv.org/abs/2304.00583">[arXiv]</a>  </li>        
        <li> 15.12.2022: I have joined as Associate Member the <a href="https://team.inria.fr/tangram/">TANGRAM team</a> at <a href="https://www.loria.fr/">INRIA Nancy/LORIA</a></li>
        <li> 27.10.2022: Co-advisor of the best Ph.D. Thesis on Computer Vision and Image Processing at <a href="https://www.natalnet.br/sibgrapi2022/awards.html">SIBGRAPI 2022</a>. Congrats <a href="http://professor.ufop.br/thiagoluange/home">Thiago Gomes</a> !</li> 
        <li> 27.10.2022: Our paper <a href="https://arxiv.org/abs/2212.09589">Learning to detect keypoints by matching</a> received the "Honourable Mention Award on PR/CV/PI at <a href="https://www.natalnet.br/sibgrapi2022/awards.html">SIBGRAPI 2022</a>
        <li> 02.08.2022: <a href="https://www.ieee-ras.org/publications/ra-l">RAL</a> paper accepted: "Calibration for polarimetric cameras" <a href="https://arxiv.org/abs/2208.13485">[arXiv]</a></li>
        <li> 05.03.2022: Our paper on "Learning Geodesic-Aware Local Features from RGB-D Images" is accepted in the journal <a href="https://www.journals.elsevier.com/computer-vision-and-image-understanding">CVIU</a> <a href="https://arxiv.org/abs/2203.12016">[arXiv]</a>  </li>
        <li> 02.01.2022: <a href="https://wacv2022.thecvf.com/">WACV 2022</a> paper accepted: "Creating and Reenacting Controllable 3D Humans with Differentiable Rendering" <a href="https://arxiv.org/abs/2110.11746">[arXiv]</a>  </li>
       <li>  12.11.2021: Outstanding Reviewer of <a href="https://3dv2021.surrey.ac.uk/program-committee/">3DV 2021</a></li>
        <li> 23.10.2021: Co-advisor of the best Brazilian M.Sc. Thesis on Computer Vision and Image Processing at <a href="https://www.inf.ufrgs.br/sibgrapi2021/awards.php#content">SIBGRAPI 2021</a>. Congrats <a href="https://jpeumesmo.github.io/">Joao Ferreira</a> !</li> 
       <li>  28.09.2021: <a href="https://proceedings.neurips.cc/">NeurIPS 2021</a> paper accepted: "Extracting Deformation-Aware Local Features by Learning to Deform" <a href="https://openreview.net/pdf?id=IQdzjJtUGFl">[arXiv]</a>  </li>
       <li> 01.09.2021: I joined as Assistant Professor (Maître de Conférences) the <a href="https://www.vibot.org">VIsion for RoBOTics - VIBOT</a> / <a href="https://imvia.u-bourgogne.fr/">ImViA</a> Lab at the <a href="https://www.u-bourgogne.fr">Université de Bourgogne</a></li>
       </ul>
    </tbody></table>
    
          <table border="0" cellpadding="20" cellspacing="0" align="center" width="100%">
      <tbody><tr>
        <td valign="middle" width="100%">
          <heading>Community Service &amp; Reviewing Activities</heading>
        </td>
      </tr>
       </tbody></table>
       <table border="0" cellpadding="0" cellspacing="0" align="center" width="100%">         
       <tbody>
          <ul class=space_list style="margin-top:20px;">
  <li style="margin-top:-20px;"> <a href="https://iccv2023.thecvf.com">ICCV 2023</a>, <a href="https://cvpr2023.thecvf.com">CVPR 2023</a>, <a href="https://aaai.org/Conferences/AAAI-23/">AAAI 2023</a>, <a href="https://wacv2023.thecvf.com/home">WACV 2023</a>, <a href="https://www.iros2023.org/">IROS 2023</a>, <a href="https://www.icra2023.org/">ICRA 2023</a>, <a href="https://www.ieee-ras.org/publications/ra-l">RAL 2023</a>, <a href="https://www.ieee-ras.org/publications/t-ro">Trans. Robotics 2023</a>, <a href="https://www.springer.com/journal/11263">IJCV 2023</a></li>		  
  <li style="margin-top:-20px;"> <a href="https://www.springer.com/journal/11263">IJCV 2022</a>, <a href="https://eccv2022.ecva.net/">ECCV 2022</a>, <a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2022</a>, <a href="https://wacv2022.thecvf.com/home">WACV 2022</a>, <a href="https://www.icra2022.org/">ICRA 2022</a>, <a href="https://www.iros2022.org/">ITSC 2022</a>, <a href="https://www.iros2022.org/">IROS 2022</a>, <a href="https://www.ieee-ras.org/publications/ra-l">RAL 2022</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">Trans. Image Processing 2022</a>, <a href="https://www.ieee-ras.org/publications/t-ro">Trans. Robotics 2022</a>, <a href="http://www.ieee-ies.org/pubs/transactions-on-industrial-informatics">Trans. Industrial Informatics 2022</a>, <a href="https://www.springer.com/journal/530">Multimedia Systems 2022</a>, <a href="https://dl.acm.org/journal/tomm">Trans. Multimedia Computing 2022</a></li>
  <li style="margin-top:-20px;"> <a href="https://3dv2021.surrey.ac.uk/">3DV 2021</a>, <a href="https://www.journals.elsevier.com/journal-of-visual-communication-and-image-representation">JVCI 2021</a>, <a href="https://2021.ieee-iv.org/">IV 2021</a>, <a href="https://www.ieee-ras.org/publications/ra-l">RAL 2021</a>, <a href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra">ICRA 2021</a>, <a href="https://2021.ieee-itsc.org/">ITSC 2021</a>,  <a href="https://www.springer.com/journal/10846">JINT 2021</a>, <a href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra">IROS 2021</a></li>
  <li style="margin-top:-20px;"> <a href="https://www.journals.elsevier.com/isprs-journal-of-photogrammetry-and-remote-sensing">ISPRS JPRS 2020</a>, <a href="https://www.journals.elsevier.com/journal-of-visual-communication-and-image-representation">JVCI 2020</a>, <a href="https://www.springer.com/journal/10846">JINT 2020</a>, <a href="https://www.ieee-ras.org/publications/ra-l">RAL 2020</a>, <a href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra/">ICRA 2020</a>, <a href="https://openaccess.thecvf.com/WACV2020">WACV 2020</a>, <a href="https://2020.ieee-iv.org/">IV 2020</a>, <a href="https://www.ieee-itsc2020.org/">ITSC 2020</a> </li>
  <li style="margin-top:-20px;"> <a href="https://www.journals.elsevier.com/journal-of-visual-communication-and-image-representation">JVCI 2019</a>, <a href="https://www.ieee-ras.org/publications/ra-l">RAL 2019</a>, <a href="https://www.icra2019.org/">ICRA 2019</a>, <a href="https://www.springer.com/journal/40313">JCAE 2019</a>, <a href="https://iv2019.org/">IV 2019</a>, <a href="https:/iv2019.org/">ITSC 2019</a>, <a href="https://ras.papercept.net/conferences/conferences/ICAR19/program/ICAR19_ProgramAtAGlanceWeb.html">ICAR 2019</a> </li>
  <li style="margin-top:-20px;"> <a href="https://www.journals.elsevier.com/isprs-journal-of-photogrammetry-and-remote-sensing">ISPRS JPRS 2018</a>, <a href="https://www.iros2018.org/">IROS 2018</a>, <a href="https://www.ieee-ras.org/publications/ra-l">RAL 2018</a>, <a href="https://iv2019.org/">IV 2018</a>, <a href="https://www.journals.elsevier.com/journal-of-visual-communication-and-image-representation">JVCI 2018</a>, <a href="https://iccv2017.thecvf.com/">ICCV 2017</a>, <a href="https://www.iros2017.org/">ICRA 2017</a>, <a href="https://www.ieee-ras.org/publications/ra-l">RAL 2017</a>, <a href="https://icinco.scitevents.org/?y=2017">ICINCO 2017</a>, <a href="https://www.iros2017.org/">IROS 2017</a> </li>
       </ul>
    </tbody></table>
    
    <table border="0" cellpadding="20" cellspacing="0" align="center" width="100%">
      <tbody><tr>
        <td valign="middle" width="100%">
          <heading>Recent Papers</heading>
        </td>
      </tr>
      </tbody></table>
      <table border="0" cellpadding="20" cellspacing="0" align="center" width="100%">         
       <tbody>

    <tr onmouseout="diverdi_stop()" onmouseover="diverdi_start()">
          <td width="25%">
            <div class="one">
            <div style="opacity: 0;" class="two" id="diverdi_image"><img src="files/dalf_cvpr.png" width="250"></div>
            <img src="files/dalf_cvpr.png", width="250">
            </div>
            <script type="text/javascript">
            function diverdi_start() {
            document.getElementById('diverdi_image').style.opacity = "1";
            }
            function diverdi_stop() {
            document.getElementById('diverdi_image').style.opacity = "0";
            }
            diverdi_stop()
            </script>
          </td>
          <td valign="top" width="75%">
            <p><a href="https://arxiv.org/pdf/2304.00583.pdf"> 
            <papertitle>Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints</papertitle></a><br>
            Guilherme Potje, Felipe Cadar, Andre Araujo, <strong>Renato Martins</strong>, and Erickson R. Nascimento <br>
            IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023 <br>
            <a href="https://arxiv.org/abs/2304.00583">arXiv</a> /
            <a href="https://www.verlab.dcc.ufmg.br/descriptors/dalf_cvpr23">project webpage</a> /
             <a href="files/potjecvpr23.bib">bibtex</a> /
            <a href="">github code</a>
            </p><p></p>
            <p> This paper proposes a learning-based keypoint detection and description approach to describe images of rigid and non-rigid objects.
              Both detection and description of keypoints are learned jointly and they are done simultaneously at test time.            
          </p>                   
        </td>
        </tr>

        <tr>
          <td>
            <br>
          </td>          
        </tr>
  
    <tr onmouseout="diverdi_stop()" onmouseover="diverdi_start()">
          <td width="25%">
            <div class="one">
            <div style="opacity: 0;" class="two" id="diverdi_image"><img src="files/keypoint_detector.png" width="250"></div>
            <img src="files/keypoint_detector.png", width="250">
            </div>
            <script type="text/javascript">
            function diverdi_start() {
            document.getElementById('diverdi_image').style.opacity = "1";
            }
            function diverdi_stop() {
            document.getElementById('diverdi_image').style.opacity = "0";
            }
            diverdi_stop()
            </script>
           
          </td>
          <td valign="top" width="75%">
            <p><a href="https://arxiv.org/pdf/2212.09589.pdf"> 
            <papertitle>Learning to Detect Good Keypoints to Match Non-Rigid Objects in RGB Images</papertitle></a><br>
            Welerson Melo, Guilherme Potje, Felipe Cadar, <strong>Renato Martins</strong>, and Erickson R. Nascimento <br>
            Conference on Graphics, Patterns and Images (<strong>SIBGRAPI</strong>), 2022 <br>
            <a href="https://arxiv.org/abs/2212.09589">arXiv</a> /
            <a href="">project webpage</a> /
             <a href="">bibtex</a> /
            <a href="https://github.com/verlab/LearningToDetect_SIBGRAPI_2022">github code</a>
            </p><p></p>
            <p> In this paper, we present a novel detection method for locating confident keypoints on images affected by non-rigid deformations. Our detector learns to locate 
              keypoints given supervision from visual correspondences, obtained by matching annotated image pairs with a predefined descriptor extractor. 
          </p>
        </td>
        </tr>

	       
      </tbody>
      </table> 
  
  <table border="0" cellpadding="20" cellspacing="0" align="center" width="100%">
      <tbody><tr>
        <td>
        <br>
        <p align="right"><font size="2">
          <a href="https://people.eecs.berkeley.edu/~barron/">Template of this webpage.</a>
          </font>
            <br>
    </td>
    </tr>
  </tbody></table>       

</body></html>
